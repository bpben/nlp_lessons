{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KD37FKDql7KB"
   },
   "source": [
    "# Week 6: NLP use-cases\n",
    "This notebook accompanies the week 6 lecture.  This week focuses on a set of interesting use-cases in NLP using the techniques we've covered in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwhFPIGPl7KC"
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "required = {'spacy', 'transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import transformers\n",
    "# this will set the device on which to train\n",
    "device = torch.device(\"cpu\")\n",
    "# if using collab, set your runtime to use GPU and use the line below\n",
    "#device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_J1R3hdWl7KI"
   },
   "source": [
    "## Unsupervised use-case: Movie genre \"clusters\"\n",
    "Though we've dealt with several labelled datasets in our class, for the most part you will not have the advantage of labels on your dataset.  However, with the techniques we've covered, we can create informative groupings that may provide answers to some questions or facilitate labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in movie descriptions from spoilers dataset\n",
    "movies = pd.read_pickle('../data/spoilers_movies.pkl.gz')\n",
    "\n",
    "# format a genre dataset, which will allow us to see how well our approach approximates the labelling\n",
    "genres = np.unique(np.concatenate(movies.genre.values))\n",
    "genre_dummies = np.zeros(shape=(len(movies), len(genres)))\n",
    "for i, r in enumerate(movies.genre):\n",
    "    for ii, g in enumerate(genres):\n",
    "        if g in r:\n",
    "            genre_dummies[i, ii] = 1\n",
    "df_genre_dummies = pd.DataFrame(genre_dummies,\n",
    "                               columns=genres,\n",
    "                               index=movies.movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Choose a technique to process movie plot summaries\n",
    "Think about some of the techniques we've already used (e.g. TFIDF, NMF, GloVe embeddings) and choose one for turning the raw summary text into a vector of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run word counts - TFIDF\n",
    "tfidf = TfidfVectorizer(min_df=0.01, stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(movies.plot_summary)\n",
    "\n",
    "# examine sets of particularly \"informative\" words (i.e. high TF, low DF)\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs.toarray(),\n",
    "            columns=tfidf.get_feature_names())\n",
    "# what's the highest-weighted per movie, look at the top 10\n",
    "tfidf_df.idxmax(axis=1).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alien seems pretty distinctive.  I imagine anything that has \"alien\" is probably sci-fi\n",
    "movies[movies.plot_summary.str.find('alien')>-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but a better measure here is comparing vectors\n",
    "tfidf_sim = cosine_similarity(tfidf_df)\n",
    "random_idx = np.random.randint(len(tfidf_sim))\n",
    "# take the most similar for this random idx (aside from self)\n",
    "movies.iloc[[random_idx,\n",
    "            tfidf_sim[random_idx].argsort()[-2]]].plot_summary.values\n",
    "# not clear if this is working, just pairwisw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run topic model - NMF\n",
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "\n",
    "# examine topic model\n",
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))\n",
    "display_components(nmf, tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i'd imagine  loading on 1 = star wars\n",
    "movies.iloc[np.argsort(nmf_vecs[:, 1])[-5:]].plot_summary.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with similarity\n",
    "nmf_sim = cosine_similarity(nmf_vecs)\n",
    "random_idx = np.random.randint(len(nmf_sim))\n",
    "# take the most similar for this random idx (aside from self)\n",
    "movies.iloc[[random_idx,\n",
    "            nmf_sim[random_idx].argsort()[-2]]].plot_summary.values\n",
    "# not clear if this is working, just pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been looking at different slices of these models/heuristics.  But the purpose, as we've covered before, is to go from unstructured text and a lot of noise, to distilled representations.  In SVM models, we used the representations as input and trained weights on each dimension.  In clustering, we'll be embedding our text in \"representation space\" and grouping those representations.\n",
    "\n",
    "The method we're using here is called K-Means clustering.  Refer to the slides for an explanation.  Suffice to say here, it's a fairly simple method for creating those groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering K-means\n",
    "clusterer = KMeans(n_clusters=5)\n",
    "clusterer.fit(nmf_vecs)\n",
    "cluster_preds = clusterer.predict(nmf_vecs)\n",
    "\n",
    "# how well did our clustering do\n",
    "print('Inertia (Sum of squared distances to cluster center):', clusterer.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how are these clusters split by genre\n",
    "# just subseting to our top 5 genres, for ease of viewing\n",
    "top_genres = df_genre_dummies.sum().nlargest(n=5).index.tolist()\n",
    "cluster_by_genre = df_genre_dummies[top_genres].groupby(cluster_preds).sum()\n",
    "cluster_by_genre\n",
    "# seems like we're seeing a lot in one cluster, yikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the % of each cluster in each genre\n",
    "cluster_by_genre.apply(lambda x: x/x.sum(), axis=1).plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to visualize this: a two-dimensional decomposition (T-SNE)\n",
    "# see https://scikit-learn.org/stable/modules/manifold.html#t-sne for more info\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "# fitting to 2 components\n",
    "tsne_decomposition = TSNE(n_components=2, random_state=42).fit_transform(nmf_vecs)\n",
    "fig, ax = plt.subplots()\n",
    "# colors based on clusters\n",
    "color_cycle = ['r', 'b', 'g', 'y', 'purple']\n",
    "ax.scatter(tsne_decomposition[:,0], tsne_decomposition[:,1],\n",
    "          c=[color_cycle[c] for c in cluster_preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with clustering\n",
    "We've tried out a specific use of KMeans with a specific set of features, but there are tons of other ways to approach this.  Spend some time experimenting with different KMeans parameters and using other sets of features.  \n",
    "\n",
    "Some example experiments:\n",
    "- Tuning the number of clusters based on lowest inertia\n",
    "- Fitting clusters to vectors of spatial information (e.g. cosine similarity)\n",
    "- Trying other featuresets (e.g. TFIDF)\n",
    "\n",
    "See if you can find a solution where the clusters seem to map on to genres to some extent, or at least where we don't see these big clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition\n",
    "The use-case we'll be covering in the slides might be considered \"semi-supervised\".  In a lot of cases, you may have an inventory of named-entities, but not necessary a fully labelled set of text.  In this use-case, we'll use our movie  dataset and a simple pattern matching implemented in SpaCy to train a NER model from scratch to recognize locations and actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use spaCy's PhraseMatcher\n",
    "# adapted from https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "# load a language model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "matcher = PhraseMatcher(en.vocab)\n",
    "inventories = {'people': [\"Barack Obama\", \"Angela Merkel\"],\n",
    "           'locations': [\"Washington, D.C.\", \"Oval Office\"]}\n",
    "for k in inventories:           \n",
    "    matcher.add(k, # label for the match \n",
    "                None, # no custom function for dealing with matches (\"callback\")\n",
    "                *[en(text) for text in inventories[k]])\n",
    "\n",
    "doc = en(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    # note: match_id is stored as an index within the vocab\n",
    "    print(match_id, en.vocab[match_id].text, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use PhraseMatcher to identify entities from the inventory in the spoilers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in reviews descriptions from spoilers dataset\n",
    "reviews = pd.read_pickle('../data/spoilers_reviews.pkl.gz')\n",
    "# an inventory of 288 academy award nominated actors \n",
    "actor_inventory = pickle.load(open('../data/actors_inventory.pkl', 'rb'))\n",
    "# a set of US cities\n",
    "city_inventory = pickle.load(open('../data/city_inventory.pkl', 'rb'))\n",
    "# just splitting off state (think why this might be appropriate!)\n",
    "city_inventory = [x.split(',')[0] for x in city_inventory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "matcher = PhraseMatcher(en.vocab)\n",
    "inventories = {'actors': actor_inventory,\n",
    "           'cities': city_inventory}\n",
    "for k in inventories:           \n",
    "    matcher.add(k, # label for the match \n",
    "                None, # no custom function for dealing with matches (\"callback\")\n",
    "                *[en(text) for text in inventories[k]])\n",
    "\n",
    "doc = en(f\"{actor_inventory[0]} is in {city_inventory[0]}\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    # note: match_id is stored as an index within the vocab\n",
    "    print(match_id, en.vocab[match_id].text, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all reviews\n",
    "parsed_summaries = [d for d in en.pipe(reviews.review_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all matches\n",
    "matches = [matcher(d) for d in parsed_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some descriptives about matching\n",
    "# total reviews with any matched\n",
    "matched_sum = np.sum([m!=[] for m in matches])\n",
    "print('% reviews with matched entities:', matched_sum/len(matches))\n",
    "# average matched per type\n",
    "inventory_matched = dict([(k, 0) for k in inventories])\n",
    "for k in inventories:\n",
    "    for m in matches:\n",
    "        if m!=[]:\n",
    "            for mm in m:\n",
    "                inventory_matched[en.vocab[mm[0]].text]+=1\n",
    "inventory_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_summaries[-1][mm[1]:mm[-1]].start_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy's NER training expects observations to be in a certain format: \n",
    "\n",
    "`(text, annotations)`\n",
    "\n",
    "Where annotations is a dictionary.  This dictionary can contain things like part-of-speech, but we'll just be including entities.  Each entity will in in this format:\n",
    "\n",
    "`(entity start character, entity end character, entity type)`\n",
    "\n",
    "So each observation will look something like this:\n",
    "\n",
    "```\n",
    "(text, \n",
    "    {'entities': [\n",
    "        (start, end, type),\n",
    "        (start, end, type),\n",
    "        ...]\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "Additionally, we'll need to deal with entities that have overlap.  We can't feed the model text that has two different entities with overlapping tokens.  So in this case, we just ignore any entities that start within the boundaries of the previous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(text, matches, excluded=[]):\n",
    "    # include list for excluded entity text\n",
    "    training = []\n",
    "    for i, t in enumerate(text):\n",
    "        entities = []\n",
    "        # dealing with overlapping entities\n",
    "        end = -1\n",
    "        for m in matches[i]:\n",
    "            # matches have token idx, need character idx\n",
    "            st = t[m[1]:m[2]].start_char\n",
    "            # if start idx is <= end idx, ignore that match\n",
    "            if st<=end:\n",
    "                continue\n",
    "            end = t[m[1]:m[2]].end_char\n",
    "            ent_type = en.vocab[m[0]].text\n",
    "            entities.append((st, end, ent_type))\n",
    "        training.append((t.text, {'entities':entities}))\n",
    "    return(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted = format_data(parsed_summaries, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the slides, we'll discuss a couple of considerations for creating a training and a test dataset.  In this case, for simplicity, we'll just split 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.7\n",
    "train_idxs = np.random.random(len(formatted))<=train_pct\n",
    "train_data = np.array(formatted)[train_idxs]\n",
    "test_data = np.array(formatted)[~train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, model_name, labels, early_stop=5, epoch=30, nlp_model=None, last_loss=np.inf):\n",
    "    if nlp_model is None:\n",
    "        # initialize model (can pass a trained model to updated)\n",
    "        nlp_model = English()\n",
    "        # ner pipeline component\n",
    "        ner = nlp_model.create_pipe('ner')\n",
    "        nlp_model.add_pipe(ner)\n",
    "        # entity types\n",
    "        for l in labels:\n",
    "            ner.add_label(l)\n",
    "        optimizer = nlp_model.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp_model.resume_training()\n",
    "    # from tutorial, creates increasing batch size\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    early_stop=5\n",
    "    last_loss = last_loss\n",
    "    for itn in range(epoch):\n",
    "        # random shuffle\n",
    "        random.shuffle(train_data)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp_model.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(\"Losses\", losses)\n",
    "        if last_loss>losses['ner']:\n",
    "            last_loss = losses['ner']\n",
    "            print('Saving model')\n",
    "            with open(model_name+'.pkl', 'wb') as f:\n",
    "                pickle.dump(nlp_model, f)\n",
    "        elif early_stop==0:\n",
    "            print('Stopping iteration')\n",
    "            break\n",
    "        else:\n",
    "            early_stop -= 1\n",
    "    return(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_data[:100], 'review_model', inventories.keys(), epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = pickle.load(open('review_model.pkl', 'rb'))\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHeO-oxtl7LH"
   },
   "outputs": [],
   "source": [
    "scorer = Scorer()\n",
    "for doc, annot in test_data[:100]:\n",
    "    doc_to_test = full_model(doc)\n",
    "    gold_text = nlp(doc)\n",
    "    gold = GoldParse(gold_text, entities=annot.get(\"entities\"))\n",
    "    scorer.score(doc_to_test, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parsed_summaries:\n",
    "    if full_model(p.text).ents!=():\n",
    "        print(full_model(p.text).ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a problem here.  It seems like the only actors being identified are male.  That's because I played a bit of a trick.  The inventory I used is all MALE award nominees.  You'd think that that's an oversight no serious designer of DS products wouldn't make.  [You'd think](https://www.wired.com/story/photo-algorithms-id-white-men-fineblack-women-not-so-much/).\n",
    "\n",
    "This is just a very simple example to show that models learn what you give them.  If I don't provide female nominees, then it won't learn to identify them.\n",
    "\n",
    "\n",
    "## Language generation with RNNs\n",
    "One a-typical use-case is being able to generate language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seqs, seq_len=200):\n",
    "    # function for adding padding to ensure all seq same length\n",
    "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) != 0:\n",
    "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
    "    return features\n",
    "\n",
    "def doc_to_index(docs, vocab):\n",
    "    # transform docs into series of indices\n",
    "    docs_idxs = []\n",
    "    for d in docs:\n",
    "        w_idxs = []\n",
    "        for w in d:\n",
    "            if w in vocab:\n",
    "                w_idxs.append(vocab[w])\n",
    "            else:\n",
    "                # unknown token = 1\n",
    "                w_idxs.append(1)\n",
    "        docs_idxs.append(w_idxs)\n",
    "    return(docs_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = '../data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "neg, pos = all_text.values()\n",
    "# join all reviews\n",
    "all_reviews = np.array(neg+pos)\n",
    "# create a (very basic) vocab of all words\n",
    "count = Counter(' '.join(all_reviews).split())\n",
    "# mimicking the countvectorizer\n",
    "count_vocab = dict((k, i) for i,k in enumerate(count.keys()))\n",
    "print(\"Size of vocab:\", len(count_vocab))\n",
    "count_vocab = dict([(v, count_vocab[v]+2) for v in count_vocab])\n",
    "count_vocab['_UNK'] = 1\n",
    "count_vocab['_PAD'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for input\n",
    "idx_docs = doc_to_index([d.split() for d in all_reviews], count_vocab)\n",
    "padded_docs = np.array(pad_sequence(idx_docs, seq_len=30))\n",
    "# outcome: the next word\n",
    "# set the last word in target = first word in input\n",
    "outcome_docs = np.array([np.append(d[1:], d[0]) for d in padded_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.reshape(padded_docs, (batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGNet(nn.Module):\n",
    "    # adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "    def __init__(self,\n",
    "                 weight_matrix=None,\n",
    "                 vocab_size=None, \n",
    "                 output_size=1,  \n",
    "                 hidden_dim=512,\n",
    "                 embedding_dim=400, \n",
    "                 n_layers=2, \n",
    "                 dropout_prob=0.5):\n",
    "        super(NLGNet, self).__init__()\n",
    "        # size of the output, in this case it's one input to one output\n",
    "        self.output_size = vocab_size\n",
    "        # number of layers (default 2) one LSTM layer, one fully-connected layer\n",
    "        self.n_layers = n_layers\n",
    "        # dimensions of our hidden state, what is passed from one time point to the next\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # initialize the representation to pass to the LSTM\n",
    "        self.embedding, embedding_dim = self.init_embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weight_matrix)\n",
    "        # LSTM layer, where the magic happens\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # forward pass of the network\n",
    "        batch_size = x.size(0)\n",
    "        # transform input\n",
    "        embeds = self.embedding(x)\n",
    "        # run input embedding + hidden state through model\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # fully connected layer\n",
    "        out = self.fc(lstm_out)\n",
    "        # return the output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_embedding(self, vocab_size, embedding_dim, weight_matrix):\n",
    "        # initializes the embedding\n",
    "        if weight_matrix is None:\n",
    "            if vocab_size is None:\n",
    "                raise ValueError('If no weight matrix, need a vocab size')\n",
    "            # if embedding is a size, initialize trainable\n",
    "            return(nn.Embedding(vocab_size, embedding_dim),\n",
    "                   embedding_dim)\n",
    "        else:\n",
    "            # otherwise use matrix as pretrained\n",
    "            weights = torch.FloatTensor(weight_matrix)\n",
    "            return(nn.Embedding.from_pretrained(weights),\n",
    "                  weights.shape[1])\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # initializes the hidden state\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'weight_matrix': None,\n",
    "               'output_size': len(count_vocab),\n",
    "                'vocab_size': len(count_vocab),\n",
    "               'hidden_dim': 512,\n",
    "               'n_layers': 2,\n",
    "               'embedding_dim': 400,\n",
    "               'dropout_prob': 0.20}\n",
    "training_params = {'learning_rate': 0.005,\n",
    "                  'epochs': 1,\n",
    "                  'batch_size': 10}\n",
    "batch_size = training_params['batch_size']\n",
    "# construct datasets for loading by PyTorch\n",
    "# in this case, we're predicting the next word\n",
    "data = TensorDataset(torch.from_numpy(padded_docs), torch.from_numpy(outcome_docs))\n",
    "loader = DataLoader(data, shuffle=True, batch_size=batch_size,\n",
    "                         drop_last=True) # this is to keep the size consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLGNet(**model_params)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                         lr=training_params['learning_rate'])\n",
    "# increasing this will make the training take a while on CPU\n",
    "# decrease to 5 if it's taking too long\n",
    "epochs = training_params['epochs']\n",
    "batch_size = training_params['batch_size']\n",
    "counter = 0\n",
    "print_every = 5\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(inputs, h)\n",
    "        loss = criterion(output.transpose(1,2), labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a review\n",
    "# create a idx-to-word translation\n",
    "idx_to_word = dict((v, k) for k,v in count_vocab.items())\n",
    "# input the words \"The\" and \"movie\"\n",
    "model.eval()\n",
    "# batch size is going to be 1, just one sequence\n",
    "h, c = model.init_hidden(1)\n",
    "words = ['This', 'movie']\n",
    "pred_words = []\n",
    "for w in words:\n",
    "    ix = torch.tensor([[count_vocab[w]]]).to(device)\n",
    "    output, (h, c) = model(ix, (h, c))\n",
    "    top_pred_word = torch.argmax(output)\n",
    "    pred_words.append(int(top_pred_word))\n",
    "while len(pred_words)<30:\n",
    "    output, (h, c) = model(top_pred_word.view(1, 1), (h, c))\n",
    "    top_pred_word = torch.argmax(output)\n",
    "    pred_words.append(int(top_pred_word))  \n",
    "print('Generated sentence:', [idx_to_word[idx] for idx in pred_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "week_5_scoping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:mainpy3] *",
   "language": "python",
   "name": "conda-env-mainpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0426dfb932f34544b41a713341f72a66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "113785db3fff4cebb333547def6a3e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_250a5b33586e446aa854e931897262b1",
       "IPY_MODEL_32c0e2488c7e4f5da4de89379f2087de"
      ],
      "layout": "IPY_MODEL_a48d098fd5f84b69bd48f781a30cd9cf"
     }
    },
    "18a058fbe9de43b8bcbbd8a9c59dfa88": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aa1af45996b48019c8edfc5b6680c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ce3f1d752e346eeaa1271f5240ea2de",
       "IPY_MODEL_d74668b6b570463baf4864f7c3c623a4"
      ],
      "layout": "IPY_MODEL_18a058fbe9de43b8bcbbd8a9c59dfa88"
     }
    },
    "250a5b33586e446aa854e931897262b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2665d51083e2495bb59f1cbfceb94cb1",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bba9944b17ed42c6bb7c7a41a57d5f32",
      "value": 442
     }
    },
    "2665d51083e2495bb59f1cbfceb94cb1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e022f78c0bb4beaa5f8adcc771b05cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32c0e2488c7e4f5da4de89379f2087de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48438770cc2493c98dd287c42a3a8d1",
      "placeholder": "​",
      "style": "IPY_MODEL_fb98c3946c3a447e8aea928fcd60787f",
      "value": " 442/442 [00:12&lt;00:00, 34.8B/s]"
     }
    },
    "37e7c6e7381d400cb5a7a1cfb07ee5a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ccb577ea8c4496f8f5da7fdb925b598": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aa01c5c96484549aacd377f3a017faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9358dac6728749958658d9fdad894be5",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0426dfb932f34544b41a713341f72a66",
      "value": 267967963
     }
    },
    "7ce3f1d752e346eeaa1271f5240ea2de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd9739bca498421cb1ce14f41acc0989",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8591a0fd56c847698965b03eb9b27c52",
      "value": 231508
     }
    },
    "8591a0fd56c847698965b03eb9b27c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9358dac6728749958658d9fdad894be5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "964be5aed4bb42fb9f1954cc31c78f44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5aa01c5c96484549aacd377f3a017faa",
       "IPY_MODEL_dda260a25b174799a87bcec9ce7d5fb3"
      ],
      "layout": "IPY_MODEL_4ccb577ea8c4496f8f5da7fdb925b598"
     }
    },
    "9eb1877f49dc4fa4a1b04c7008db029d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a48d098fd5f84b69bd48f781a30cd9cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20ebae009a043e589d85a01f86769c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bba9944b17ed42c6bb7c7a41a57d5f32": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d74668b6b570463baf4864f7c3c623a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37e7c6e7381d400cb5a7a1cfb07ee5a3",
      "placeholder": "​",
      "style": "IPY_MODEL_2e022f78c0bb4beaa5f8adcc771b05cd",
      "value": " 232k/232k [00:00&lt;00:00, 693kB/s]"
     }
    },
    "dda260a25b174799a87bcec9ce7d5fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9eb1877f49dc4fa4a1b04c7008db029d",
      "placeholder": "​",
      "style": "IPY_MODEL_b20ebae009a043e589d85a01f86769c6",
      "value": " 268M/268M [00:09&lt;00:00, 27.4MB/s]"
     }
    },
    "f48438770cc2493c98dd287c42a3a8d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb98c3946c3a447e8aea928fcd60787f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd9739bca498421cb1ce14f41acc0989": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
