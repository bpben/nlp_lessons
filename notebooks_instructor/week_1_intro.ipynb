{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 1: Introduction and Overview\n",
    "This notebook accompanies the week 1 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'spacy-transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Natural Language Tool Kit vs SpaCy\n",
    "For this course, we will mainly be using [SpaCy]().  There are a [number of other]() NLP libraries and probably one of the best known is the [Natural Language Tool Kit (NLTK)]().  SpaCy and NLTK both are very powerful, but here I'll show a couple of reasons why I prefer SpaCy.\n",
    "\n",
    "Note: You will not be able to run these on your own without installing NLTK.  Since it's not used in the rest of the course, I'm not configuring it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "text = 'We are doing NLP.'\n",
    "doc = en(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "doc = word_tokenize(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time comparison\n",
    "%timeit en(text)\n",
    "%timeit nltk.tokenize.casual_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing: lower case and removing non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# spacy\n",
    "%timeit  [x.lower_ for x in en(text) if x.is_alpha]\n",
    "# nltk (one possible way)\n",
    "%timeit nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can see both libraries are very powerful.  But SpaCy's syntax is a bit simpler and it's generally a bit faster.  Feel free to experiment with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This section shows some of the considerations to make when tokenizing your data.\n",
    "\n",
    "Token = \"Useful semantic unit\"\n",
    "\n",
    "But what does that mean? This section will detail some considerations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# importing different languages in spacy\n",
    "# blank English model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "# blank Chinese model\n",
    "# to run, will need to install jieba tokenizer (optional)\n",
    "#!pip install jieba\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "zh = spacy.lang.zh.Chinese()\n",
    "zh_text = '我们正在做NLP。'\n",
    "print('Tokenize in Chinese:', [x.text for x in zh(zh_text)])\n",
    "print('Tokenize in English:', [x.text for x in en(zh_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "text = 'We are doing NLP.'\n",
    "print('Base python: ', text.lower())\n",
    "print('SpaCy:', [x.lower_ for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# handling non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print([x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but what about contractions?\n",
    "text = \"We're doing NLP.\"\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print('Just removing punctuation:', re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print('Removing non-alpha', [x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the is_alpha flag is False for any tokens that have non-alpha characters.  We'll look into a better way for dealing with contractions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a tokenizer\n",
    "In this exercise, you will make a function that uses spaCy's base English model to tokenize a dataset according to specific parameters.  The functions will take a list of documents and output a list of tokens.  In this case we're interested in outputting strings, rather than spaCy tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "en = English()\n",
    "\n",
    "def tokenize_base(docs, model=en):\n",
    "    # tokenizer that just parses using spaCy's base model\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.text for t in parsed])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha(docs, model=en):\n",
    "    # tokenizer that lowercases and removes any non-alpha character\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if t.is_alpha])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha_url(docs, model=en):\n",
    "    # tokenizer that lowercases, removes any non-alpha character and removes urls\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "Though word tense can sometimes carry with it a lot of useful information, a lot of time it might be useful to reduce words to their common root.  For example, the word \"be\" has various forms like \"are\", \"is\", \"been\".  We might not want our vocabulary to contain all these forms and rather count them all as instances of \"be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course.'\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Dealing with stop words involves making some pretty impactful decisions with your data.  Refer to the slides for details.  Here, we just remove stop words based on [spaCy's default set](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = English()\n",
    "text = 'In June 2020, I took a course at Harvard Extension School in Cambridge.'\n",
    "print(text)\n",
    "print([x.text for x in en(text) if not x.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-standard tokens (e.g. named-entities)\n",
    "In text, some some n-grams should not be treated as a concatenation of unigrams.  For example, New York City is fundamentally different from the individual words \"new\", \"york\" and \"city\".\n",
    "\n",
    "Here we attempt to deal with some of these non-standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls\n",
    "# base python\n",
    "# regex from textacy: https://github.com/chartbeat-labs/textacy\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out these courses: https://www.summer.harvard.edu/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))\n",
    "# spacy\n",
    "print([x for x in en(text) if not x.like_url])\n",
    "# spacy - replace with a standard token\n",
    "print(['-URL-' if x.like_url else x for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named-entities\n",
    "# read in English model with tagging/entity pipeline components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course at Harvard starting July 19th, 2020'\n",
    "parsed = nlp(text)\n",
    "# look at the individual tokens\n",
    "tokens = [t for t in parsed]\n",
    "print(tokens)\n",
    "# look at the identified named-entities and their types\n",
    "for e in parsed.ents:\n",
    "    print(e, type(e), e.label_, spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A comprehensive tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_full(text_data, stop_words=True, alpha_only=False, entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "A very basic way to use a sanitized list of tokens is to do a word count.  This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "tokenized = [simple_tokenizer(doc) for doc in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base python: create an make use of a Counter object\n",
    "counts = [Counter(d) for d in tokenized]\n",
    "print('List of counts:', counts)\n",
    "# sum together all counts\n",
    "all_counts = Counter()\n",
    "for d in tokenized:\n",
    "    all_counts += Counter(d)\n",
    "print(counts)\n",
    "print('\\nCombined count:', all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "# result is the same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sentiment analysis with word counts\n",
    "Imagine you are a hot dog restaurant owner and you want to analyze a corpus of reviews from diners to see whether people generally think your hot dogs are \"good\" or \"bad\".  Specifically, you're going to count up the number of times the word \"good\" and word \"bad\" appears.  Depending on how you process the text, you will arrive at different conclusions.  Try a couple ways to see what I mean.\n",
    "\n",
    "You might also want to think about whether all the reviews are relevant.  Those sorts of choices may also affect your results.  Is there an automatic way you can remove non-relevant reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['These hot dogs are really good.',\n",
    "          'These hot dogs are really bad.',\n",
    "          'Good hot dogs!',\n",
    "          'The hot dogs pair well with a Good Humor bar.',\n",
    "          \"I didn't eat anything, I felt bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "en = English()\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "v = cv.fit_transform(reviews).toarray()\n",
    "count_df = pd.DataFrame(v, columns=cv.get_feature_names())\n",
    "print('Good count:', count_df['good'].sum())\n",
    "print('Bad count:', count_df['bad'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=False)\n",
    "v = cv.fit_transform(reviews).toarray()\n",
    "count_df = pd.DataFrame(v, columns=cv.get_feature_names())\n",
    "print('Good count:', count_df['good'].sum())\n",
    "print('Bad count:', count_df['bad'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to advanced models\n",
    "In this section, we'll be setting up some of the requirements for the more advanced techniques we will cover later in the course.  Particularly, we'll be working with:\n",
    "\n",
    "- [huggingface's transformers library](https://github.com/huggingface/transformers)\n",
    "- [spaCy-transformers (based on the above)](https://github.com/explosion/spacy-transformers)\n",
    "\n",
    "These require some additional downloads.  For these examples you'll need:\n",
    "\n",
    "[BERT uncased large model](https://github.com/google-research/bert)\n",
    "\n",
    "SpaCy's medium English model (with word vectors from GloVe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install extra models\n",
    "# only need to run this once per session\n",
    "#!python -m spacy download en_trf_bertbaseuncased_lg\n",
    "#!python -m spacy download en_core_web_md\n",
    "# you will likely need to restart your kernel to load the models\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT model\n",
    "nlp_bert = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "# load medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spaCy Doc-Span-Token structure is still in place\n",
    "text = \"This is a sentence.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    parsed = model(text)\n",
    "    print(type(parsed), type(parsed[0]))\n",
    "    # but the vector representation is different\n",
    "    print('Vector shape:', parsed.vector.shape)\n",
    "    # documents under the transformer model have additional attributes\n",
    "    print(parsed._.trf_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adapted from spaCy's example\n",
    "# compare two different senses of the word \"Apple\" based on similarity\n",
    "# similarity: higher values = more similar\n",
    "apple_org = \"Apple sold fewer iPhones this quarter.\"\n",
    "apple_food = \"Apple pie is delicious.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    print('Similarity between senses:', model(apple_org)[0].similarity(model(apple_food)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mainpy3] *",
   "language": "python",
   "name": "conda-env-mainpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
