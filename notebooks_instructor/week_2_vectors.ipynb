{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 2: From tokens to vectors\n",
    "This notebook accompanies the week 2 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this to avoid some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'numpy', 'pandas'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts revisited\n",
    "Let's remind ourselves how sklearn's CountVectorizer worked (from last week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as expected.  Why don't we try this on Assignment 1's dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = '../data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15365),\n",
       " ('a', 7548),\n",
       " ('and', 6978),\n",
       " ('to', 6780),\n",
       " ('of', 6402),\n",
       " ('is', 4952),\n",
       " ('it', 4354),\n",
       " ('i', 4248),\n",
       " ('in', 4203),\n",
       " ('this', 3837)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this on negative reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "neg_vectors = cv.fit_transform(neg).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), neg_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 17182),\n",
       " ('and', 8905),\n",
       " ('a', 8133),\n",
       " ('of', 7942),\n",
       " ('to', 6658),\n",
       " ('is', 5793),\n",
       " ('in', 5093),\n",
       " ('it', 4553),\n",
       " ('i', 3943),\n",
       " ('that', 3521)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do it for positive reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "pos_vectors = cv.fit_transform(pos).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), pos_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words aren't particularly informative about the content.  Sklearn's CountVectorizer has some additional options that may lead to somewhat more informative frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 2297), ('film', 1797), ('like', 1085), ('just', 984), ('bad', 668), ('did', 637), ('good', 632), ('really', 620), ('time', 560), ('does', 521)]\n",
      "[('film', 2122), ('movie', 1904), ('like', 891), ('good', 811), ('just', 713), ('great', 654), ('story', 625), ('time', 604), ('really', 539), ('does', 508)]\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    cv = CountVectorizer(tokenizer=simple_tokenizer, min_df=0.01, max_df=0.9,\n",
    "                        stop_words='english')\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "    word_count = dict(zip(cv.get_feature_names(), vectors.sum(axis=0)))\n",
    "    # get the top 10 words\n",
    "    print(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it seems like we'd have to tweak these thresholds a lot and carefully choose our stop words.  Is there a more standard way to extract the most informative words from documents?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "See the slides for more information on this.  In this section we'll show how TF-IDF is essentially just a weighting of the count vectors.  We'll then use sklearn's built-in TfidfVectorizer on our sentiment corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  great  movie  the  was\n",
       "0    0     1      0      1    1    1\n",
       "1    1     0      0      1    1    1\n",
       "2    0     0      1      1    1    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF).\n",
    "\n",
    "The formula sklearn uses is a bit different from the textbook:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$\n",
    "\n",
    "Where $N$ is the number of documents.  It also normalizes this value to account for different size vectors (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad     good    great     movie       the       was\n",
       "0  0.00000  0.69903  0.00000  0.412859  0.412859  0.412859\n",
       "1  0.69903  0.00000  0.00000  0.412859  0.412859  0.412859\n",
       "2  0.00000  0.00000  0.69903  0.412859  0.412859  0.412859"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the discriminative words (i.e. bad, good, great) have higher weight than the non-discriminative words.  \n",
    "\n",
    "We see this at the document level, but is there a way we could get some kind of aggregate measure of discriminative words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the top 3 discriminative words\n",
    "Use the dataset above to try and identify the words that, across the corpus, are particularly representative of content.\n",
    "\n",
    "Hint: Think about what a weight of zero versus weight of non-zero means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_words(tfidf_df):\n",
    "    return(tfidf_df[tfidf_df>0].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad      0.699030\n",
       "good     0.699030\n",
       "great    0.699030\n",
       "movie    0.412859\n",
       "the      0.412859\n",
       "was      0.412859\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_words(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that on our movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children    0.151333\n",
      "monster     0.152135\n",
      "dr          0.153720\n",
      "cute        0.156171\n",
      "island      0.157478\n",
      "space       0.165138\n",
      "theater     0.165205\n",
      "nt          0.167994\n",
      "game        0.189159\n",
      "the         0.267655\n",
      "dtype: float64\n",
      "and         0.153424\n",
      "oscar       0.154922\n",
      "dance       0.155299\n",
      "edge        0.159386\n",
      "rock        0.162227\n",
      "western     0.165590\n",
      "ryan        0.167666\n",
      "japanese    0.172789\n",
      "dr          0.177205\n",
      "the         0.274663\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    # adding in a minimum document frequency, so words need to occur at least somewhat often\n",
    "    tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, min_df=0.02)\n",
    "    vectors = tfidf.fit_transform(corpus).toarray()\n",
    "    tfidf_df = pd.DataFrame(vectors, columns=tfidf.get_feature_names())\n",
    "    # get representative words\n",
    "    tfidf_word_count = top_tfidf_words(tfidf_df)\n",
    "    # get the top 10 words\n",
    "    print(tfidf_word_count.sort_values().iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are somewhat useful aggregate measures.  But most of the information in TF-IDF is document-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "See the slides for detail on this.  Sklearn has an implementation that's useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.75, 0.75],\n",
       "       [0.75, 1.  , 0.75],\n",
       "       [0.75, 0.75, 1.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# cosine similarity without a second argument (y) compares all docs to one another\n",
    "cosine_similarity(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the diagonal axis is a documents similarity to itself.  Off diagonal are the similarities between doc x and doc y.  Each of these docs has basically the same words except for good, bad and great.  So the similarity between them is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cosine similarity two ways\n",
    "- Using the movie reviews dataset combine all reviews, but keep an indicator to know which are positive and which are negative.\n",
    "- Get count vectors and tf-idf vectors\n",
    "- Select a review and find most similar and least similar for both methods\n",
    "- Get the average distance between positive and negative reviews\n",
    "\n",
    "Tips: \n",
    "- You don't need to convert any of this to DataFrames to do this work.  It'll be faster if you don't!\n",
    "- You can use `np.fill_diagonal` to fill the diagonal entries for the similarity matrix with values\n",
    "\n",
    "Think about: Why do we fit the vectors on all reviews, rather than separately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "all_reviews = neg+pos\n",
    "# indicator where is first positive\n",
    "first_pos = len(neg)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# get vectors\n",
    "count_vecs = cv.fit_transform(all_reviews)\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "# get similarities\n",
    "count_sims = cosine_similarity(count_vecs)\n",
    "tfidf_sims = cosine_similarity(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The premise of this movie is ugghhhh. The guy is married and yet everyone on this site seems to think, \"Yeah, this is funny, cute, and a good movie.\" What the Hell?!?! What is funny about immature girls fornicating with a married man with a new baby? What is cute about the fact that he is cheating on his wife? What have been wrong with them finding some teenage boys to have sex with before starting college? Noooo, that is not good enough, the guy has to be married, off-limits, off-the-market, that's the one we gotta have. Dumb-ass GIRLS! Then one of the girls decides that she \"loves\" the guy. No, she just \"loves\" the way he makes her feel. Two of the girls are having fun with it, they think it is funny and no one seems to have any moral problems with what they are doing. It just shameless, but yeah this is all good with everyone one this website. The dark-haired girl even has the audacity to have her dad pick her up from the guys house, under the ruse of baby sitting. This is a morally disgusting movie and where is the wife? Poor woman working and paying the bills while he screws the baby sitter.\n",
      "The premise of this movie is ugghhhh. The guy is married and yet everyone on this site seems to think, \"Yeah, this is funny, cute, and a good movie.\" What the Hell?!?! What is funny about immature girls fornicating with a married man with a new baby? What is cute about the fact that he is cheating on his wife? What have been wrong with them finding some teenage boys to have sex with before starting college? Noooo, that is not good enough, the guy has to be married, off-limits, off-the-market, that's the one we gotta have. Dumb-ass GIRLS! Then one of the girls decides that she \"loves\" the guy. No, she just \"loves\" the way he makes her feel. Two of the girls are having fun with it, they think it is funny and no one seems to have any moral problems with what they are doing. It just shameless, but yeah this is all good with everyone one this website. The dark-haired girl even has the audacity to have her dad pick her up from the guys house, under the ruse of baby sitting. This is a morally disgusting movie and where is the wife? Poor woman working and paying the bills while he screws the baby sitter.\n"
     ]
    }
   ],
   "source": [
    "# sample one, find most similar - count vectors\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(count_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that there was too much action in the end? Don't you think that too? There was romance, adventure that just like told me to put 9 to this movie but action place was too long. I liked Reeve a bit. I didn't understand why did he have to die. I thought that one of the girls gonna die too but my lucky! No one else who I liked didn't die! How about you? What did you liked? I saw the movie twice actually. And after that I bought that too. It was worth it! Who did you liked best (person)?. The book was really, really, really cool. And the actresses and actors too. Everything was perfect....... What was the song name in the end? Will someone answer my questions too... PLEASE, please please?\n",
      "I think that there was too much action in the end? Don't you think that too? There was romance, adventure that just like told me to put 9 to this movie but action place was too long. I liked Reeve a bit. I didn't understand why did he have to die. I thought that one of the girls gonna die too but my lucky! No one else who I liked didn't die! How about you? What did you liked? I saw the movie twice actually. And after that I bought that too. It was worth it! Who did you liked best (person)?. The book was really, really, really cool. And the actresses and actors too. Everything was perfect....... What was the song name in the end? Will someone answer my questions too... PLEASE, please please?\n"
     ]
    }
   ],
   "source": [
    "# sample one, find most similar - tfidf\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(tfidf_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg-to-neg: 0.5334077646265442 neg-to-pos: 0.5314326255417756 pos-to-pos: 0.5369682327485035\n",
      "neg-to-neg: 0.1297415905734085 neg-to-pos: 0.1236987648556773 pos-to-pos: 0.12412758408737891\n"
     ]
    }
   ],
   "source": [
    "# compare positive to negative average distance\n",
    "for s_matrix in [count_sims, tfidf_sims]:\n",
    "    print('neg-to-neg:', s_matrix[:first_pos, :first_pos].mean(axis=1).mean(),\n",
    "          'neg-to-pos:', s_matrix[:first_pos, first_pos:].mean(axis=1).mean(),\n",
    "          'pos-to-pos:', s_matrix[first_pos:, first_pos:].mean(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 277 µs, sys: 10 µs, total: 287 µs\n",
      "Wall time: 283 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cosine_similarity([[1]*3,\n",
    "                   [0.5]*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 µs, sys: 59 µs, total: 591 µs\n",
      "Wall time: 563 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cosine_similarity([[1]*300,\n",
    "                   [0.5]*300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion here is that tf-idf seems to work better in actually getting similar reviews.  However, the distance shows that, on average, a negative review's vocabulary isn't that much different from a positive review's.  \n",
    "\n",
    "But maybe this conclusion might change if we can distill some of the information from the counts into more meaningful dimensions.  That brings us to:\n",
    "\n",
    "## Topic models: Non-negative Matrix Factorization and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, excluding standard english stop words\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "count_vecs = cv.fit_transform(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# basic configuration\n",
    "nmf = NMF(n_components=n_components)\n",
    "# NMF requires tfidf, not word counts\n",
    "# same syntax as vectorizer\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "# LDA uses word counts\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NMF and LDA provide a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13268742e-03 0.00000000e+00 3.66257230e-03 ... 1.89289887e-05\n",
      "  6.78943418e-04 6.78943418e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.20339776e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.42497778e-04\n",
      "  2.33935929e-03 2.33935929e-03]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.35654186e-04 0.00000000e+00 0.00000000e+00 ... 2.82807814e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.28668220e-03 ... 7.24422708e-05\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating performance, both methods use different ways to quantify the loss from using the topic model versus the actual data.  (In the matrix formulation, $UV$ rather than $X$).  For NMF, it's reconstruction error, which is more directly the difference between the matrix decomposition and the actual data.  For LDA, it uses [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound), which is a too complicated to explain here.  In both, higher values means worse performance.  They can't be compared to one another, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.814189917141164 6797.597038462212\n"
     ]
    }
   ],
   "source": [
    "print(nmf.reconstruction_err_, lda.bound_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film films good seen does\n",
      "Topic 1:\n",
      "movie watch good movies time\n",
      "Topic 2:\n",
      "man life story family young\n",
      "Topic 3:\n",
      "horror effects special budget gore\n",
      "Topic 4:\n",
      "br money music audience thing\n",
      "Topic 5:\n",
      "great best good action love\n",
      "Topic 6:\n",
      "did like just people really\n",
      "Topic 7:\n",
      "book read story novel character\n",
      "Topic 8:\n",
      "series episode episodes tv season\n",
      "Topic 9:\n",
      "bad acting good worst movies\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film like movie just good\n",
      "Topic 1:\n",
      "movie film like just really\n",
      "Topic 2:\n",
      "did movie film like funny\n",
      "Topic 3:\n",
      "film like just story good\n",
      "Topic 4:\n",
      "film man films story like\n",
      "Topic 5:\n",
      "film like great good movie\n",
      "Topic 6:\n",
      "movie good like just time\n",
      "Topic 7:\n",
      "movie film like does characters\n",
      "Topic 8:\n",
      "movie film like br just\n",
      "Topic 9:\n",
      "film movie like does just\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF seems to have come up with some reasonable topics, but LDA doesn't seem to work particularly well here.  It may make sense to try some additional token processing and see how that affects what we get out of the topic modelling process.\n",
    "\n",
    "### Exercise: Tokenization decisions and topic models\n",
    "Using the tokenizer from week 1 or your own tokenizer, explore how your tokenization decisions up stream might affect your results downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize_full(all_reviews, entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if passing a list of tokens to a vectorizer, you can use the following syntax\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vecs = tfidf.fit_transform(tokenized)\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "count_vecs = cv.fit_transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film see watch good time\n",
      "Topic 1:\n",
      "movie watch see think time\n",
      "Topic 2:\n",
      "love life story family young\n",
      "Topic 3:\n",
      "like scene go look people\n",
      "Topic 4:\n",
      "series episode watch season funny\n",
      "Topic 5:\n",
      "br music money play audience\n",
      "Topic 6:\n",
      "great good actor cast acting\n",
      "Topic 7:\n",
      "book read story character novel\n",
      "Topic 8:\n",
      "bad acting see terrible waste\n",
      "Topic 9:\n",
      "game video play level like\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "movie good film time story\n",
      "Topic 1:\n",
      "film like story movie br\n",
      "Topic 2:\n",
      "film movie like good time\n",
      "Topic 3:\n",
      "movie character film like good\n",
      "Topic 4:\n",
      "movie film like watch good\n",
      "Topic 5:\n",
      "film movie time scene like\n",
      "Topic 6:\n",
      "film movie character play story\n",
      "Topic 7:\n",
      "film good like movie great\n",
      "Topic 8:\n",
      "film play good like find\n",
      "Topic 9:\n",
      "movie like scene great film\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: Using text features for prediction\n",
    "Week 1 and all of the above focused on creating features from text.  The tokenization decisions are mainly deterministic, they output what you tell them to output.  The topic models step more into the \"learning\" aspect of analysis, where you ask the algorithm to find a decomposition that fits the data.  The output of interest in this case is a reconstruction of the data.\n",
    "\n",
    "But what if you're not interested in reconstructing the data, but rather predicting a specific outcome? In that case, you'll need some amount of data with that outcome specified.  From there, you can ask the machine to learn the relationship between text features and the outcome.  This is, roughly, the idea behind supervised learning.\n",
    "\n",
    "In this section, we'll introduce how to use text features in predicting an outcome.  Assignment 2 will focus on how to convert the work you did on Assignment 1 into a supervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I love these hot dogs',\n",
    "          'I hate these hot dogs',\n",
    "          'These hot dogs are really good',\n",
    "          'These hot dogs are really bad']\n",
    "is_positive = [1, 0, 1, 0]\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "count_vecs = cv.fit_transform(reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit/predict on full dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(count_vecs, is_positive)\n",
    "svc.predict(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try predicting on new observatons\n",
    "new_obs = [\"I love these!\",\n",
    "           \"I don't love these hot dogs\"]\n",
    "new_count = cv.transform(new_obs)\n",
    "svc.predict(new_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model with its current features is 50% accurate on our new observations.  That's not great.  But this is a very small vocabulary and a small dataset.  Why don't we try with our movie reviews?\n",
    "\n",
    "Remember: This dataset is already labelled.  A review is either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1754,) (745,)\n"
     ]
    }
   ],
   "source": [
    "# create binary indicator for positive review\n",
    "is_positive = np.array([0]*len(neg)+[1]*len(pos))\n",
    "# sample random 70% for fitting model (training)\n",
    "# 30% will be simulating \"new observations\" (testing)\n",
    "pct_sample = 0.7\n",
    "train_bool = np.random.random(len(all_reviews))<pct_sample\n",
    "reviews_train = np.array(all_reviews)[train_bool]\n",
    "reviews_test = np.array(all_reviews)[~train_bool]\n",
    "is_positive_train = is_positive[train_bool]\n",
    "is_positive_test = is_positive[~train_bool]\n",
    "print(reviews_train.shape, reviews_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit count vectorizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "train_vecs = cv.fit_transform(reviews_train).toarray()\n",
    "test_vecs = cv.transform(reviews_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/predict on training dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(train_vecs, is_positive_train)\n",
    "train_preds = svc.predict(train_vecs)\n",
    "test_preds = svc.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8348993288590604\n"
     ]
    }
   ],
   "source": [
    "# scoring accuracy\n",
    "print('Train accuracy:', accuracy_score(is_positive_train, train_preds))\n",
    "print('Test accuracy:', accuracy_score(is_positive_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is usually much higher than test accuracy.  That makes sense: The vocabulary and the model are fit to the training data.  But the bigger concern is how the model performs on data we haven't seen yet.  Test accuracy gives us a measure of that.  83% is not bad...but it could be better.\n",
    "\n",
    "Assignment 2 is all about trying to boost this accuracy by trying out some the vectorization methods we've gone through here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
