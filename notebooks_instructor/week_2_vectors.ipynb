{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 2: From tokens to vectors\n",
    "This notebook accompanies the week w lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this to avoid some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'numpy', 'pandas'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "See the slides for detail on this.  Sklearn has an implementation that's useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.75, 0.75],\n",
       "       [0.75, 1.  , 0.75],\n",
       "       [0.75, 0.75, 1.  ]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# cosine similarity without a second argument (y) compares all docs to one another\n",
    "cosine_similarity(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the diagonal axis is a documents similarity to itself.  Off diagonal are the similarities between doc x and doc y.  Each of these docs has basically the same words except for good, bad and great.  So the similarity between them is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts revisited\n",
    "Let's remind ourselves how sklearn's CountVectorizer worked (from last week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])\n",
    "\n",
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as expected.  Why don't we try this on Assignment 1's dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = '../data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15365),\n",
       " ('a', 7548),\n",
       " ('and', 6978),\n",
       " ('to', 6780),\n",
       " ('of', 6402),\n",
       " ('is', 4952),\n",
       " ('it', 4354),\n",
       " ('i', 4248),\n",
       " ('in', 4203),\n",
       " ('this', 3837)]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this on negative reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "neg_vectors = cv.fit_transform(neg).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), neg_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 17182),\n",
       " ('and', 8905),\n",
       " ('a', 8133),\n",
       " ('of', 7942),\n",
       " ('to', 6658),\n",
       " ('is', 5793),\n",
       " ('in', 5093),\n",
       " ('it', 4553),\n",
       " ('i', 3943),\n",
       " ('that', 3521)]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do it for positive reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "pos_vectors = cv.fit_transform(pos).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), pos_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words aren't particularly informative about the content.  Sklearn's CountVectorizer has some additional options that may lead to somewhat more informative frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'nt'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 2297), ('film', 1797), ('like', 1085), ('just', 984), ('bad', 668), ('did', 637), ('good', 632), ('really', 620), ('time', 560), ('does', 521)]\n",
      "[('film', 2122), ('movie', 1904), ('like', 891), ('good', 811), ('just', 713), ('great', 654), ('story', 625), ('time', 604), ('really', 539), ('does', 508)]\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    cv = CountVectorizer(tokenizer=simple_tokenizer, min_df=0.01, max_df=0.9,\n",
    "                        stop_words='english')\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "    word_count = dict(zip(cv.get_feature_names(), vectors.sum(axis=0)))\n",
    "    # get the top 10 words\n",
    "    print(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it seems like we'd have to tweak these thresholds a lot and carefully choose our stop words.  Is there a more standard way to extract the most informative words from documents?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "See the slides for more information on this.  In this section we'll show how TF-IDF is essentially just a weighting of the count vectors.  We'll then use sklearn's built-in TfidfVectorizer on our sentiment corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  great  movie  the  was\n",
       "0    0     1      0      1    1    1\n",
       "1    1     0      0      1    1    1\n",
       "2    0     0      1      1    1    1"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF).\n",
    "\n",
    "The formula sklearn uses is a bit different from the textbook:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$\n",
    "\n",
    "Where $N$ is the number of documents.  It also normalizes this value to account for different size vectors (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad     good    great     movie       the       was\n",
       "0  0.00000  0.69903  0.00000  0.412859  0.412859  0.412859\n",
       "1  0.69903  0.00000  0.00000  0.412859  0.412859  0.412859\n",
       "2  0.00000  0.00000  0.69903  0.412859  0.412859  0.412859"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the discriminative words (i.e. bad, good, great) have higher weight than the non-discriminative words.  \n",
    "\n",
    "We see this at the document level, but is there a way we could get some kind of aggregate measure of discriminative words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the top 3 discriminative words\n",
    "Use the dataset above to try and identify the words that, across the corpus, are particularly representative of content.\n",
    "\n",
    "Hint: Think about what a weight of zero versus weight of non-zero means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_words(tfidf_df):\n",
    "    return(tfidf_df[tfidf_df>0].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad      0.699030\n",
       "good     0.699030\n",
       "great    0.699030\n",
       "movie    0.412859\n",
       "the      0.412859\n",
       "was      0.412859\n",
       "dtype: float64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_words(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that on our movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children    0.151333\n",
      "monster     0.152135\n",
      "dr          0.153720\n",
      "cute        0.156171\n",
      "island      0.157478\n",
      "space       0.165138\n",
      "theater     0.165205\n",
      "nt          0.167994\n",
      "game        0.189159\n",
      "the         0.267655\n",
      "dtype: float64\n",
      "and         0.153424\n",
      "oscar       0.154922\n",
      "dance       0.155299\n",
      "edge        0.159386\n",
      "rock        0.162227\n",
      "western     0.165590\n",
      "ryan        0.167666\n",
      "japanese    0.172789\n",
      "dr          0.177205\n",
      "the         0.274663\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    # adding in a minimum document frequency, so words need to occur at least somewhat often\n",
    "    tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, min_df=0.02)\n",
    "    vectors = tfidf.fit_transform(corpus).toarray()\n",
    "    tfidf_df = pd.DataFrame(vectors, columns=tfidf.get_feature_names())\n",
    "    # get representative words\n",
    "    tfidf_word_count = top_tfidf_words(tfidf_df)\n",
    "    # get the top 10 words\n",
    "    print(tfidf_word_count.sort_values().iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more informative. We can probably do better here.\n",
    "\n",
    "### Exercise: Tease out more discriminative words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are somewhat useful aggregate measures.  But most of the information in TF-IDF is document-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cosine similarity two ways\n",
    "- Using the movie reviews dataset combine all reviews, but keep an indicator to know which are positive and which are negative.\n",
    "- Get count vectors and tf-idf vectors\n",
    "- Select a review and find most similar and least similar for both methods\n",
    "- Get the average distance between positive and negative reviews\n",
    "\n",
    "Tips: \n",
    "- You don't need to convert any of this to DataFrames to do this work.  It'll be faster if you don't!\n",
    "- You can use `np.fill_diagonal` to fill the diagonal entries for the similarity matrix with values\n",
    "\n",
    "Think about: Why do we fit the vectors on all reviews, rather than separately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "all_reviews = neg+pos\n",
    "# indicator where is first positive\n",
    "first_pos = len(neg)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# get vectors\n",
    "count_vecs = cv.fit_transform(all_reviews)\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "# get similarities\n",
    "count_sims = cosine_similarity(count_vecs)\n",
    "tfidf_sims = cosine_similarity(tfidf_vecs)\n",
    "# Replace self-similarity with negative\n",
    "np.fill_diagonal(count_sims, -1)\n",
    "np.fill_diagonal(tfidf_sims, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw Five Fingers at the Drive-In in...what, 1973, '74? It was the the first Kung-Fu movie I'd ever seen and I was greatly entertained. I recently bought it on DVD and watched it again. I was greatly entertained the second time, too. I believe this is probably the one most Kung-Fu movies are modeled after. Rival Schools, different styles, revenge, \"white hat\" good guys and \"black hat\" bad guys. They even threw in the Japanese (VERY bad guys) styles of Karate and Judo. I remember being amused by the dubbing dialog, along the lines of \"Hey You! You are a very bad guy!\" and \"They should not get away with this! I will have a go at this bad crowd!\" This time it wasn't so distracting, I guess I'm used to it. If you have even the slightest appreciation of this genre, this is one you should see.\n",
      "Have I seen a worse movie? No I can't say that I have. This was pathetic. If the director is still alive: 1. He shouldn't be. 2. He should be ashamed. 3. God, how I would like to take out my 2 completely wasted hours of time on his a$$.<br /><br />To give you guys a few pointers of the \"film\": <br /><br />1. (I'm a male) and I would rather give myself a papercut on the opening of my urethra before viewing this again (seriously).<br /><br />2. It does have a few known names in it (Casper Van Dien, Erika Eleniak, Coolio). They don't help, and their careers in cinema after this \"film\" are officially over by the way.<br /><br />3. The dialog is the worst I've ever heard. \"I want to ejaculate on your bozonkas.\"? What kind of writer did they have on this film? Was he still using hooked-on-phonics and just got his letters mixed up to make these horrible sentences?, or was he trying to get the Director killed by the few people who saw this? <br /><br />4. Watch this \"film\" backwards. Because I PROMISE you that you do not want to watch it forwards.<br /><br />5. This \"film\" would make Helen Keller get up and walk out of the theater.<br /><br />6. The set of the movie looks like an adult sized McDonald's playplace. I was just waiting for this so called \"Dracula\" to fall in the ball pit at some time in the movie.<br /><br />7. Also, I like that in the year 3000 they still have headsets with wires that go to their mouth. No bluetooth, no wireless headsets, no chips placed in the brain, but they use headsets borrowed from a telemarketing agency that went out of business in 1983(Nice job Set director on this one. Real professional. I hope you're currently unemployed and reading this.) <br /><br />8. I don't know who was in charge of special effects, but I could have done better in my backyard with my VHS camcorder that doesn't have a battery.<br /><br />9. I was a devout Catholic before this \"film\". But since viewing it, I know there is not a God, because if there was, he wouldn't have let this film be produced. I am now an atheist.<br /><br />10. I'll be honest I can't talk about the ending. Last time I tried to explain it I fell into a coma.<br /><br />Folks however bored you get, however curious(or brave) you are, however many laughs you THINK you will get out of this movie, please DO NOT WATCH THIS. It has literally ruined my life. AVOID AT ALL COSTS!<br /><br />Comment to the director: I hate you. You have ruined my life. After viewing this I feel empty inside. My wife and kids have left me and hate me because I couldn't speak or hardly move after seeing this. I lost my job, my dignity, and above all my pride. I will never forgive you in this life or the next(which is not looking good from my newfound beliefs) .\n"
     ]
    }
   ],
   "source": [
    "# sample one, find most similar - count vectors\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(count_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Roger Corman rip-off assembled for what appears to be virtually zero budget. All of the special effects were originally used in \"Battle Beyond the Stars\", and I suspect a fair amount of the props, costumes and sets were re-used from other sources as well. The story seems to have been written around these elements, so this isn't really a movie as much as it's a recycling project. Third-rate \"Star Wars\" junk wasn't needed then or now.\n",
      "As several posters have \"hinted,\" this is a sorry \"Star Wars\" ripoff. Now if you're going to rip off \"Star Wars,\" at least do it right; \"Battlestar Galactica\" did, and there were a few other space operas that didn't do a bad job of it, but this is definitely not one of them. David Mendenhall, the juvenile lead, actually isn't too bad, though he goes overboard on the \"cute\" factor every so often. Vince Edwards hasn't improved much as an actor since his \"Ben Casey\" days; if anything, he's even more wooden than he was ten. The other performances are nothing to write home about, either. Even worse are the special effects; the best you can say about them is that they're lousy. It's glaringly obvious that the \"aliens\" are simply actors wearing rubber masks with a little foam or latex slopped on them, and the \"battle\" scenes between Edwards' raiders and the aliens are poorly staged and badly shot. A very weak effort from Roger Corman. Skip it.\n"
     ]
    }
   ],
   "source": [
    "# sample one, find most similar - tfidf\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(tfidf_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg-to-neg: 0.5317857046103236 neg-to-pos: 0.5314326255417756 pos-to-pos: 0.5353884539175398\n",
      "neg-to-neg: 0.12811953055718792 neg-to-pos: 0.1236987648556773 pos-to-pos: 0.12254780525641525\n"
     ]
    }
   ],
   "source": [
    "# compare positive to negative average distance\n",
    "for s_matrix in [count_sims, tfidf_sims]:\n",
    "    print('neg-to-neg:', s_matrix[:first_pos, :first_pos].mean(axis=1).mean(),\n",
    "          'neg-to-pos:', s_matrix[:first_pos, first_pos:].mean(axis=1).mean(),\n",
    "          'pos-to-pos:', s_matrix[first_pos:, first_pos:].mean(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 285 µs, sys: 12 µs, total: 297 µs\n",
      "Wall time: 290 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cosine_similarity([[1]*3,\n",
    "                   [0.5]*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 385 µs, sys: 56 µs, total: 441 µs\n",
      "Wall time: 411 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cosine_similarity([[1]*300,\n",
    "                   [0.5]*300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion here is that tf-idf seems to work better in actually getting similar reviews.  However, the distance shows that, on average, a negative review's vocabulary isn't that much different from a positive review's.  \n",
    "\n",
    "But maybe this conclusion might change if we can distill some of the information from the counts into more meaningful dimensions.  That brings us to:\n",
    "\n",
    "## Topic models: Non-negative Matrix Factorization and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, excluding standard english stop words\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "count_vecs = cv.fit_transform(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# basic configuration\n",
    "nmf = NMF(n_components=n_components)\n",
    "# NMF requires tfidf, not word counts\n",
    "# same syntax as vectorizer\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "# LDA uses word counts\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NMF and LDA provide a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.13268742e-03 0.00000000e+00 3.66257230e-03 ... 1.89289887e-05\n",
      "  6.78943418e-04 6.78943418e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.20339776e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.42497778e-04\n",
      "  2.33935929e-03 2.33935929e-03]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.35654186e-04 0.00000000e+00 0.00000000e+00 ... 2.82807814e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.28668220e-03 ... 7.24422708e-05\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating performance, both methods use different ways to quantify the loss from using the topic model versus the actual data.  (In the matrix formulation, $UV$ rather than $X$).  For NMF, it's reconstruction error, which is more directly the difference between the matrix decomposition and the actual data.  For LDA, it uses [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound), which is a too complicated to explain here.  In both, higher values means worse performance.  They can't be compared to one another, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.814189917141164 6797.597038462212\n"
     ]
    }
   ],
   "source": [
    "print(nmf.reconstruction_err_, lda.bound_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film films good seen does\n",
      "Topic 1:\n",
      "movie watch good movies time\n",
      "Topic 2:\n",
      "man life story family young\n",
      "Topic 3:\n",
      "horror effects special budget gore\n",
      "Topic 4:\n",
      "br money music audience thing\n",
      "Topic 5:\n",
      "great best good action love\n",
      "Topic 6:\n",
      "did like just people really\n",
      "Topic 7:\n",
      "book read story novel character\n",
      "Topic 8:\n",
      "series episode episodes tv season\n",
      "Topic 9:\n",
      "bad acting good worst movies\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film like movie just good\n",
      "Topic 1:\n",
      "movie film like just really\n",
      "Topic 2:\n",
      "did movie film like funny\n",
      "Topic 3:\n",
      "film like just story good\n",
      "Topic 4:\n",
      "film man films story like\n",
      "Topic 5:\n",
      "film like great good movie\n",
      "Topic 6:\n",
      "movie good like just time\n",
      "Topic 7:\n",
      "movie film like does characters\n",
      "Topic 8:\n",
      "movie film like br just\n",
      "Topic 9:\n",
      "film movie like does just\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF seems to have come up with some reasonable topics, but LDA doesn't seem to work particularly well here.  It may make sense to try some additional token processing and see how that affects what we get out of the topic modelling process.\n",
    "\n",
    "### Exercise: Tokenization decisions and topic models\n",
    "Using the tokenizer from week 1 or your own tokenizer, explore how your tokenization decisions up stream might affect your results downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize_full(all_reviews, entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if passing a list of tokens to a vectorizer, you can use the following syntax\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vecs = tfidf.fit_transform(tokenized)\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "count_vecs = cv.fit_transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film see watch good time\n",
      "Topic 1:\n",
      "movie watch see think time\n",
      "Topic 2:\n",
      "love life story family young\n",
      "Topic 3:\n",
      "like scene go look people\n",
      "Topic 4:\n",
      "series episode watch season funny\n",
      "Topic 5:\n",
      "br music money play audience\n",
      "Topic 6:\n",
      "great good actor cast acting\n",
      "Topic 7:\n",
      "book read story character novel\n",
      "Topic 8:\n",
      "bad acting see terrible waste\n",
      "Topic 9:\n",
      "game video play level like\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "movie good film time story\n",
      "Topic 1:\n",
      "film like story movie br\n",
      "Topic 2:\n",
      "film movie like good time\n",
      "Topic 3:\n",
      "movie character film like good\n",
      "Topic 4:\n",
      "movie film like watch good\n",
      "Topic 5:\n",
      "film movie time scene like\n",
      "Topic 6:\n",
      "film movie character play story\n",
      "Topic 7:\n",
      "film good like movie great\n",
      "Topic 8:\n",
      "film play good like find\n",
      "Topic 9:\n",
      "movie like scene great film\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: Using text features for prediction\n",
    "Week 1 and all of the above focused on creating features from text.  The tokenization decisions are mainly deterministic, they output what you tell them to output.  The topic models step more into the \"learning\" aspect of analysis, where you ask the algorithm to find a decomposition that fits the data.  The output of interest in this case is a reconstruction of the data.\n",
    "\n",
    "But what if you're not interested in reconstructing the data, but rather predicting a specific outcome? In that case, you'll need some amount of data with that outcome specified.  From there, you can ask the machine to learn the relationship between text features and the outcome.  This is, roughly, the idea behind supervised learning.\n",
    "\n",
    "In this section, we'll introduce how to use text features in predicting an outcome.  Assignment 2 will focus on how to convert the work you did on Assignment 1 into a supervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I love these hot dogs',\n",
    "          'I hate these hot dogs',\n",
    "          'These hot dogs are really good',\n",
    "          'These hot dogs are really bad']\n",
    "is_positive = [1, 0, 1, 0]\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "count_vecs = cv.fit_transform(reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit/predict on full dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(count_vecs, is_positive)\n",
    "svc.predict(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try predicting on new observatons\n",
    "new_obs = [\"I love these!\",\n",
    "           \"I don't love these hot dogs\"]\n",
    "new_count = cv.transform(new_obs)\n",
    "svc.predict(new_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model with its current features is 50% accurate on our new observations.  That's not great.  But this is a very small vocabulary and a small dataset.  Why don't we try with our movie reviews?\n",
    "\n",
    "Remember: This dataset is already labelled.  A review is either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1754,) (745,)\n"
     ]
    }
   ],
   "source": [
    "# create binary indicator for positive review\n",
    "is_positive = np.array([0]*len(neg)+[1]*len(pos))\n",
    "# sample random 70% for fitting model (training)\n",
    "# 30% will be simulating \"new observations\" (testing)\n",
    "pct_sample = 0.7\n",
    "train_bool = np.random.random(len(all_reviews))<pct_sample\n",
    "reviews_train = np.array(all_reviews)[train_bool]\n",
    "reviews_test = np.array(all_reviews)[~train_bool]\n",
    "is_positive_train = is_positive[train_bool]\n",
    "is_positive_test = is_positive[~train_bool]\n",
    "print(reviews_train.shape, reviews_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit count vectorizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "train_vecs = cv.fit_transform(reviews_train).toarray()\n",
    "test_vecs = cv.transform(reviews_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/predict on training dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(train_vecs, is_positive_train)\n",
    "train_preds = svc.predict(train_vecs)\n",
    "test_preds = svc.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8348993288590604\n"
     ]
    }
   ],
   "source": [
    "# scoring accuracy\n",
    "print('Train accuracy:', accuracy_score(is_positive_train, train_preds))\n",
    "print('Test accuracy:', accuracy_score(is_positive_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is usually much higher than test accuracy.  That makes sense: The vocabulary and the model are fit to the training data.  But the bigger concern is how the model performs on data we haven't seen yet.  Test accuracy gives us a measure of that.  83% is not bad...but it could be better.\n",
    "\n",
    "Assignment 2 is all about trying to boost this accuracy by trying out some the vectorization methods we've gone through here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
