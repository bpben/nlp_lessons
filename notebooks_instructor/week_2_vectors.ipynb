{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 2: From tokens to vectors\n",
    "This notebook accompanies the week 2 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this to avoid some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'numpy', 'pandas'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts revisited\n",
    "Let's remind ourselves how sklearn's CountVectorizer worked (from last week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as expected.  Why don't we try this on Assignment 1's dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = '../data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this on negative reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "neg_vectors = cv.fit_transform(neg).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), neg_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it for positive reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "pos_vectors = cv.fit_transform(pos).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), pos_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words aren't particularly informative about the content.  Sklearn's CountVectorizer has some additional options that may lead to somewhat more informative frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    cv = CountVectorizer(tokenizer=simple_tokenizer, min_df=0.01, max_df=0.9,\n",
    "                        stop_words='english')\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "    word_count = dict(zip(cv.get_feature_names(), vectors.sum(axis=0)))\n",
    "    # get the top 10 words\n",
    "    print(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it seems like we'd have to tweak these thresholds a lot and carefully choose our stop words.  Is there a more standard way to extract the most informative words from documents?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "See the slides for more information on this.  In this section we'll show how TF-IDF is essentially just a weighting of the count vectors.  We'll then use sklearn's built-in TfidfVectorizer on our sentiment corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF).\n",
    "\n",
    "The formula sklearn uses is a bit different from the textbook:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$\n",
    "\n",
    "Where $N$ is the number of documents.  It also normalizes this value to account for different size vectors (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the discriminative words (i.e. bad, good, great) have higher weight than the non-discriminative words.  \n",
    "\n",
    "We see this at the document level, but is there a way we could get some kind of aggregate measure of discriminative words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the top 3 discriminative words\n",
    "Use the dataset above to try and identify the words that, across the corpus, are particularly representative of content.\n",
    "\n",
    "Hint: Think about what a weight of zero versus weight of non-zero means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_words(tfidf_df):\n",
    "    return(tfidf_df[tfidf_df>0].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf_words(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that on our movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    # adding in a minimum document frequency, so words need to occur at least somewhat often\n",
    "    tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, min_df=0.02)\n",
    "    vectors = tfidf.fit_transform(corpus).toarray()\n",
    "    tfidf_df = pd.DataFrame(vectors, columns=tfidf.get_feature_names())\n",
    "    # get representative words\n",
    "    tfidf_word_count = top_tfidf_words(tfidf_df)\n",
    "    # get the top 10 words\n",
    "    print(tfidf_word_count.sort_values().iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are somewhat useful aggregate measures.  But most of the information in TF-IDF is document-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "See the slides for detail on this.  Sklearn has an implementation that's useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# cosine similarity without a second argument (y) compares all docs to one another\n",
    "cosine_similarity(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the diagonal axis is a documents similarity to itself.  Off diagonal are the similarities between doc x and doc y.  Each of these docs has basically the same words except for good, bad and great.  So the similarity between them is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cosine similarity two ways\n",
    "- Using the movie reviews dataset combine all reviews, but keep an indicator to know which are positive and which are negative.\n",
    "- Get count vectors and tf-idf vectors\n",
    "- Select a review and find most similar and least similar for both methods\n",
    "- Get the average distance between positive and negative reviews\n",
    "\n",
    "Tips: \n",
    "- You don't need to convert any of this to DataFrames to do this work.  It'll be faster if you don't!\n",
    "- You can use `np.fill_diagonal` to fill the diagonal entries for the similarity matrix with values\n",
    "\n",
    "Think about: Why do we fit the vectors on all reviews, rather than separately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "all_reviews = neg+pos\n",
    "# indicator where is first positive\n",
    "first_pos = len(neg)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# get vectors\n",
    "count_vecs = cv.fit_transform(all_reviews)\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "# get similarities\n",
    "count_sims = cosine_similarity(count_vecs)\n",
    "tfidf_sims = cosine_similarity(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample one, find most similar - count vectors\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(count_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample one, find most similar - tfidf\n",
    "random_idx = np.random.randint(len(all_reviews))\n",
    "print(all_reviews[random_idx]+'\\n'+\n",
    "      all_reviews[np.argmax(tfidf_sims[random_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare positive to negative average distance\n",
    "for s_matrix in [count_sims, tfidf_sims]:\n",
    "    print('neg-to-neg:', s_matrix[:first_pos, :first_pos].mean(axis=1).mean(),\n",
    "          'neg-to-pos:', s_matrix[:first_pos, first_pos:].mean(axis=1).mean(),\n",
    "          'pos-to-pos:', s_matrix[first_pos:, first_pos:].mean(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion here is that tf-idf seems to work better in actually getting similar reviews.  However, the distance shows that, on average, a negative review's vocabulary isn't that much different from a positive review's.  \n",
    "\n",
    "But maybe this conclusion might change if we can distill some of the information from the counts into more meaningful dimensions.  That brings us to:\n",
    "\n",
    "## Topic models: Non-negative Matrix Factorization and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, excluding standard english stop words\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "count_vecs = cv.fit_transform(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# basic configuration\n",
    "nmf = NMF(n_components=n_components)\n",
    "# NMF requires tfidf, not word counts\n",
    "# same syntax as vectorizer\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "# LDA uses word counts\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02823817, 0.06582423, 0.00120355, 0.        ,\n",
       "       0.01070525, 0.03576207, 0.01071412, 0.01198975, 0.01922563])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_vecs.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NMF and LDA provide a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating performance, both methods use different ways to quantify the loss from using the topic model versus the actual data.  (In the matrix formulation, $UV$ rather than $X$).  For NMF, it's reconstruction error, which is more directly the difference between the matrix decomposition and the actual data.  For LDA, it uses [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound), which is a too complicated to explain here.  In both, higher values means worse performance.  They can't be compared to one another, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.reconstruction_err_, lda.bound_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF seems to have come up with some reasonable topics, but LDA doesn't seem to work particularly well here.  It may make sense to try some additional token processing and see how that affects what we get out of the topic modelling process.\n",
    "\n",
    "### Exercise: Tokenization decisions and topic models\n",
    "Using the tokenizer from week 1 or your own tokenizer, explore how your tokenization decisions up stream might affect your results downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize_full(all_reviews, entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if passing a list of tokens to a vectorizer, you can use the following syntax\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vecs = tfidf.fit_transform(tokenized)\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "count_vecs = cv.fit_transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: Using text features for prediction\n",
    "Week 1 and all of the above focused on creating features from text.  The tokenization decisions are mainly deterministic, they output what you tell them to output.  The topic models step more into the \"learning\" aspect of analysis, where you ask the algorithm to find a decomposition that fits the data.  The output of interest in this case is a reconstruction of the data.\n",
    "\n",
    "But what if you're not interested in reconstructing the data, but rather predicting a specific outcome? In that case, you'll need some amount of data with that outcome specified.  From there, you can ask the machine to learn the relationship between text features and the outcome.  This is, roughly, the idea behind supervised learning.\n",
    "\n",
    "In this section, we'll introduce how to use text features in predicting an outcome.  Assignment 2 will focus on how to convert the work you did on Assignment 1 into a supervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I love these hot dogs',\n",
    "          'I hate these hot dogs',\n",
    "          'These hot dogs are really good',\n",
    "          'These hot dogs are really bad']\n",
    "is_positive = [1, 0, 1, 0]\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "count_vecs = cv.fit_transform(reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit/predict on full dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(count_vecs, is_positive)\n",
    "svc.predict(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try predicting on new observatons\n",
    "new_obs = [\"I love these!\",\n",
    "           \"I don't love these hot dogs\"]\n",
    "new_count = cv.transform(new_obs)\n",
    "svc.predict(new_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model with its current features is 50% accurate on our new observations.  That's not great.  But this is a very small vocabulary and a small dataset.  Why don't we try with our movie reviews?\n",
    "\n",
    "Remember: This dataset is already labelled.  A review is either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary indicator for positive review\n",
    "is_positive = np.array([0]*len(neg)+[1]*len(pos))\n",
    "# sample random 70% for fitting model (training)\n",
    "# 30% will be simulating \"new observations\" (testing)\n",
    "pct_sample = 0.7\n",
    "train_bool = np.random.random(len(all_reviews))<pct_sample\n",
    "reviews_train = np.array(all_reviews)[train_bool]\n",
    "reviews_test = np.array(all_reviews)[~train_bool]\n",
    "is_positive_train = is_positive[train_bool]\n",
    "is_positive_test = is_positive[~train_bool]\n",
    "print(reviews_train.shape, reviews_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit count vectorizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "train_vecs = cv.fit_transform(reviews_train).toarray()\n",
    "test_vecs = cv.transform(reviews_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/predict on training dataset\n",
    "svc = LinearSVC()\n",
    "svc.fit(train_vecs, is_positive_train)\n",
    "train_preds = svc.predict(train_vecs)\n",
    "test_preds = svc.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring accuracy\n",
    "print('Train accuracy:', accuracy_score(is_positive_train, train_preds))\n",
    "print('Test accuracy:', accuracy_score(is_positive_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is usually much higher than test accuracy.  That makes sense: The vocabulary and the model are fit to the training data.  But the bigger concern is how the model performs on data we haven't seen yet.  Test accuracy gives us a measure of that.  83% is not bad...but it could be better.\n",
    "\n",
    "Assignment 2 is all about trying to boost this accuracy by trying out some the vectorization methods we've gone through here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mainpy3] *",
   "language": "python",
   "name": "conda-env-mainpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
